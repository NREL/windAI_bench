{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "import math\n",
    "import joblib\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import altair as alt\n",
    "\n",
    "## Set options\n",
    "# display all columns in pandas dataframe\n",
    "pd.set_option('display.max_columns',None)\n",
    "# allow unlimited number of rows in altair datasets\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "\n",
    "# Set data path here\n",
    "data_path = 'wind_plant_data.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in 2D and 3D onehot encoded representations of the data \n",
    "with h5py.File(data_path, 'r') as hf:\n",
    "    full_2d_arr = hf['/One-hot Encoded/2D Representation/Full 2D array'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the Full 2D array to a pandas dataframe with column headers for exploratory data analysis\n",
    "\n",
    "max_num_turbs = 200\n",
    "column_names = ['layout', 'scenario', 'opt_yaw', 'num_turbines', 'hub_h', 'rotor_d', \n",
    "                'wind_dir', 'wind_speed', 'turbulence']\n",
    "column_template = ['t_{:03d}', 't_X_{:03d}', 't_Y_{:03d}', 't_ws_{:03d}', 't_yaw_{:03d}', 't_power_{:03d}']\n",
    "for c in column_template:\n",
    "    for i in range(max_num_turbs):\n",
    "        column_names.append(c.format(i))\n",
    "\n",
    "df = pd.DataFrame(data=full_2d_arr, columns=column_names)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify features and labels\n",
    "\n",
    "# Note: t_ws_XXX is provided for completeness but is not used for this model\n",
    "features = ['num_turbines', 'wind_dir', 'wind_speed', 'turbulence'] \\\n",
    "              + ['t_{:03d}'.format(i) for i in range(max_num_turbs)]\\\n",
    "              + ['t_X_{:03d}'.format(i) for i in range(max_num_turbs)] \\\n",
    "              + ['t_Y_{:03d}'.format(i) for i in range(max_num_turbs)] \\\n",
    "              + ['t_yaw_{:03d}'.format(i) for i in range(max_num_turbs)]\n",
    "print('Features to train on:\\n', features)\n",
    "\n",
    "labels = ['t_power_{:03d}'.format(i) for i in range(max_num_turbs)]\n",
    "print('\\nLabels to predict:\\n', labels)\n",
    "\n",
    "local_ws = ['t_ws_{:03d}'.format(i) for i in range(max_num_turbs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General statistics of the data\n",
    "df[features+labels].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for plant level features to visualized distributions\n",
    "hist_plant_features = [features[1], features[4], features[5]]\n",
    "num_cols = 4\n",
    "num_rows = math.ceil(len(hist_plant_features)/num_cols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(5*num_cols,5*num_rows))\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    if idx >= len(hist_plant_features):\n",
    "        ax.remove()\n",
    "        break\n",
    "    ax.hist(df[hist_plant_features[idx]], edgecolor='black')\n",
    "    # sns.histplot(df[hist_plant_features[idx]], stat=\"percent\", ax=ax)\n",
    "    ax.set_xlabel(hist_plant_features[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for turbine X and Y locations features to visualized distributions\n",
    "t_X_idx_start = features.index('t_X_000')\n",
    "t_X_idx_end = features.index('t_X_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_X_features = features[t_X_idx_start:t_X_idx_end+1]\n",
    "\n",
    "t_Y_idx_start = features.index('t_Y_000')\n",
    "t_Y_idx_end = features.index('t_Y_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_Y_features = features[t_Y_idx_start:t_Y_idx_end+1]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "axes[0].hist(df[hist_X_features].unstack().dropna(), edgecolor='black')\n",
    "axes[0].set_xlabel('Turbine X Locations')\n",
    "axes[1].hist(df[hist_Y_features].unstack().dropna(), edgecolor='black')\n",
    "axes[1].set_xlabel('Turbine Y Locations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for turbine yaw angle features to visualized distributions\n",
    "t_yaw_idx_start = features.index('t_yaw_000')\n",
    "t_yaw_idx_end = features.index('t_yaw_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_yaw_features = features[t_yaw_idx_start:t_yaw_idx_end+1]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "axes[0].hist(df[df['opt_yaw'] == 0][hist_yaw_features].unstack().dropna(), edgecolor='black')\n",
    "axes[0].set_xlabel('Randomized Yaw Angles')\n",
    "axes[1].hist(df[df['opt_yaw'] == 1][hist_yaw_features].unstack().dropna(), edgecolor='black')\n",
    "axes[1].set_xlabel('Optimized Yaw Angles')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for local turbine wind speeds to visualized distributions\n",
    "t_ws_idx_start = local_ws.index('t_ws_000')\n",
    "t_ws_idx_end = local_ws.index('t_ws_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_ws_features = local_ws[t_ws_idx_start:t_ws_idx_end+1]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "axes[0].hist(df[df['opt_yaw'] == 0][hist_ws_features].unstack().dropna(), edgecolor='black')\n",
    "axes[0].set_xlabel('Local Turbine Wind Speeds')\n",
    "axes[1].hist(df[df['opt_yaw'] == 1][hist_ws_features].unstack().dropna(), edgecolor='black')\n",
    "axes[1].set_xlabel('Local Turbine Wind Speeds (optimized yaw)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for turbine power outputs features to visualized distributions\n",
    "t_power_idx_start = labels.index('t_power_000')\n",
    "t_power_idx_end = labels.index('t_power_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_power_features = labels[t_power_idx_start:t_power_idx_end+1]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "axes[0].hist(df[df['opt_yaw'] == 0][hist_power_features].unstack().dropna(), edgecolor='black')\n",
    "axes[0].set_xlabel('Turbine Power')\n",
    "axes[1].hist(df[df['opt_yaw'] == 1][hist_power_features].unstack().dropna(), edgecolor='black')\n",
    "axes[1].set_xlabel('Turbine Power (optimized yaw)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot for visualizing Plant Power vs plant wind speed\n",
    "df_alt = df[['opt_yaw','wind_speed']]\n",
    "df_alt['norm_plant_power'] =  df[hist_power_features].sum(axis=1).div(df['num_turbines']*3.4e6)\n",
    "df_alt['opt_yaw'] = df_alt['opt_yaw'].astype(bool)\n",
    "\n",
    "df_alt.head()\n",
    "\n",
    "non_opt = alt.Chart(df_alt[df_alt['opt_yaw']==0]).mark_circle().encode(\n",
    "    x = alt.X('wind_speed:Q', title='Plant Level Wind Speed'),\n",
    "    y = alt.Y('norm_plant_power:Q', title='Normalized Plant Power'),\n",
    "    color = alt.Color('opt_yaw:N', title='Optimized Yaw Angles')\n",
    ")\n",
    "\n",
    "opt = alt.Chart(df_alt[df_alt['opt_yaw']==1]).mark_circle(opacity=0.3).encode(\n",
    "    x = alt.X('wind_speed:Q', title='Plant Level Wind Speed'),\n",
    "    y = alt.Y('norm_plant_power:Q', title='Normalized Plant Power'),\n",
    "    color = alt.Color('opt_yaw:N', title='Optimized Yaw Angles')\n",
    ")\n",
    "\n",
    "non_opt + opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatterplot to visualize plant layout and turbine power\n",
    "t_X_idx_start = features.index('t_X_000')\n",
    "t_X_idx_end = features.index('t_X_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_X_features = features[t_X_idx_start:t_X_idx_end+1]\n",
    "\n",
    "t_Y_idx_start = features.index('t_Y_000')\n",
    "t_Y_idx_end = features.index('t_Y_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_Y_features = features[t_Y_idx_start:t_Y_idx_end+1]\n",
    "\n",
    "t_power_idx_start = labels.index('t_power_000')\n",
    "t_power_idx_end = labels.index('t_power_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_power_features = labels[t_power_idx_start:t_power_idx_end+1]\n",
    "\n",
    "ridx = random.randint(0,len(df)-1)\n",
    "\n",
    "x_locs = df[hist_X_features].iloc[ridx].values.T\n",
    "y_locs = df[hist_Y_features].iloc[ridx].values.T\n",
    "t_powers = df[hist_power_features].iloc[ridx].values.T\n",
    "\n",
    "df_alt2 = pd.DataFrame([x_locs,y_locs,t_powers]).T\n",
    "df_alt2.columns = ['x_loc','y_loc','t_powers']\n",
    "df_alt2.dropna(inplace=True)\n",
    "df_alt2['t_powers'] = df_alt2['t_powers'].div(1e6)\n",
    "\n",
    "alt.Chart(df_alt2).mark_circle().encode(\n",
    "    x = alt.X('x_loc:Q', title='X Location'),\n",
    "    y = alt.Y('y_loc:Q', title='Y Location'),\n",
    "    color = alt.Color('t_powers:Q', scale=alt.Scale(scheme='viridis'), legend=alt.Legend(format=',.3f'), title='Turbine Power (MW)'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatterplot to visualize plant layout and turbine power\n",
    "t_X_idx_start = features.index('t_X_000')\n",
    "t_X_idx_end = features.index('t_X_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_X_features = features[t_X_idx_start:t_X_idx_end+1]\n",
    "\n",
    "t_Y_idx_start = features.index('t_Y_000')\n",
    "t_Y_idx_end = features.index('t_Y_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_Y_features = features[t_Y_idx_start:t_Y_idx_end+1]\n",
    "\n",
    "t_power_idx_start = labels.index('t_power_000')\n",
    "t_power_idx_end = labels.index('t_power_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "hist_power_features = labels[t_power_idx_start:t_power_idx_end+1]\n",
    "\n",
    "ridx = random.randint(0,len(df)-1)\n",
    "\n",
    "x_locs = df[hist_X_features].iloc[ridx].values.T\n",
    "y_locs = df[hist_Y_features].iloc[ridx].values.T\n",
    "t_powers = df[hist_power_features].iloc[ridx].values.T\n",
    "\n",
    "df_alt2 = pd.DataFrame([x_locs,y_locs,t_powers]).T\n",
    "df_alt2.columns = ['x_loc','y_loc','t_powers']\n",
    "df_alt2.dropna(inplace=True)\n",
    "df_alt2['t_powers'] = df_alt2['t_powers'].div(1e6)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x=df_alt2['x_loc'], y=df_alt2['y_loc'], c=df_alt2['t_powers'], edgecolor='k')\n",
    "xlim = plt.gca().get_xlim()\n",
    "ylim = plt.gca().get_ylim()\n",
    "plt.xlim(np.minimum(xlim[0], ylim[0]), np.maximum(xlim[1], ylim[1]))\n",
    "plt.ylim(np.minimum(xlim[0], ylim[0]), np.maximum(xlim[1], ylim[1]))\n",
    "plt.gca().set_aspect(1.)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Power (MW)')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_2d_to_df():\n",
    "    ## Generate column headers list and data type dictionary for instantiating pandas dataframe\n",
    "    # Generate turbine specific feature column headers ranging between 000-max_num_turbs, = one-hot encoded values for each turbine \n",
    "    t_dummy_vars = ['t_{x}'.format(x=str(y).zfill(3)) for y in range(max_num_turbs)]\n",
    "    t_X_vars = ['t_X_{x}'.format(x=str(y).zfill(3)) for y in range(max_num_turbs)]\n",
    "    t_Y_vars = ['t_Y_{x}'.format(x=str(y).zfill(3)) for y in range(max_num_turbs)]\n",
    "    t_yaw_vars = ['t_yaw_{x}'.format(x=str(y).zfill(3)) for y in range(max_num_turbs)]\n",
    "    t_ws_vars = ['t_ws_{x}'.format(x=str(y).zfill(3)) for y in range(max_num_turbs)]\n",
    "    t_power_vars = ['t_power_{x}'.format(x=str(y).zfill(3)) for y in range(max_num_turbs)]\n",
    "    # Define plant level variables\n",
    "    plant_vars   = ['layout',           # index for LayoutXXX\n",
    "                    'scenario',         # index for ScenarioXXX\n",
    "                    'opt_yaw',          # binary if sample is yaw optimized\n",
    "                    'num_turbines',     # number of turbines in the layout\n",
    "                    'hub_h',            \n",
    "                    'rotor_d',\n",
    "                    'wind_dir',         # wind direction\n",
    "                    'wind_speed',       \n",
    "                    'turbulence',]\n",
    "    # create list to hold all column names\n",
    "    column_names = []\n",
    "    column_names.extend(plant_vars)\n",
    "    column_names.extend(t_dummy_vars)\n",
    "    column_names.extend(t_X_vars)\n",
    "    column_names.extend(t_Y_vars)\n",
    "    column_names.extend(t_yaw_vars)\n",
    "    column_names.extend(t_ws_vars)\n",
    "    column_names.extend(t_power_vars)\n",
    "\n",
    "    # Define datatypes for each column\n",
    "    plant_vars_type = ['int32',\n",
    "                    'int32',\n",
    "                    'int8',\n",
    "                    'int32',\n",
    "                    'float32',\n",
    "                    'float32',\n",
    "                    'float32',\n",
    "                    'float32',\n",
    "                    'float32']\n",
    "    turb_vars_type = ['float32' for i in range(max_num_turbs)]\n",
    "\n",
    "    data_type_dict = dict(zip(plant_vars, plant_vars_type))\n",
    "    data_type_dict.update(dict(zip(t_dummy_vars,['int8' for i in range(max_num_turbs)])))\n",
    "    data_type_dict.update(dict(zip(t_X_vars,turb_vars_type)))\n",
    "    data_type_dict.update(dict(zip(t_Y_vars,turb_vars_type)))\n",
    "    data_type_dict.update(dict(zip(t_yaw_vars,turb_vars_type)))\n",
    "    data_type_dict.update(dict(zip(t_ws_vars,turb_vars_type)))\n",
    "    data_type_dict.update(dict(zip(t_power_vars,turb_vars_type)))\n",
    "\n",
    "    ## Convert the Full 2D array to a pandas dataframe with column headers for exploratory data analysis\n",
    "    df = pd.DataFrame(data=full_2d_arr, columns=column_names)\n",
    "    df = df.astype(data_type_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, split=(0.6, 0.2, 0.2), random_seed=888):\n",
    "    \"\"\"\n",
    "    Splits X/y into training, validation, and testing sets based on the\n",
    "    specified split.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Collection of sample features to train on\n",
    "    y : np.ndarray\n",
    "        Collection of sample labels to be predicted (turbine power)\n",
    "    split : tuple\n",
    "        The percentage allocation of training, validation, and test datasets\n",
    "        Default: (0.6, 0.2, 0.2)\n",
    "    random_seed : int\n",
    "        The seed for the random number generator\n",
    "        Default: 888\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test : np.ndarrays\n",
    "        The X and y training, validation, and test datasets \n",
    "    \"\"\"\n",
    "    # Input checking\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"X and y lengths don't match ({len(X)} != {len(y)})\")\n",
    "    if len(split) != 3:\n",
    "        raise ValueError(\"Invalid split, expected 3 percentages (training, validation, test)\")\n",
    "    if sum(split) != 1:\n",
    "        raise ValueError(f\"Invalid split {split}, percentages must sum to 1.0\")\n",
    "    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n",
    "        raise TypeError(\"X and y must be numpy arrays\")\n",
    "\n",
    "    # Initial split for testing data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=split[2], random_state=random_seed, stratify=X[:,0]         # stratify based on # of optimized yaw simulations in each set (first column in X)\n",
    "    )\n",
    "\n",
    "    # Further split for validation data\n",
    "    val_size = split[1] / (1 - split[2])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, random_state=random_seed, stratify=X_train[:,0]   # stratify based on # of optimized yaw simulations in each set (first column in X_train)\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, Load, and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in 2D and 3D onehot encoded representations of the data\n",
    "with h5py.File(data_path, 'r') as hf:\n",
    "    full_2d_arr = hf['/One-hot Encoded/2D Representation/Full 2D array'][()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "df = load_full_2d_to_df()\n",
    "\n",
    "# Fill NaN values with zero\n",
    "df.fillna(0,inplace=True)\n",
    "\n",
    "# Save feature and label names\n",
    "features = list(df.columns[2:-2*max_num_turbs])\n",
    "# print('Features to train on:\\n', features)\n",
    "labels = list(df.columns[-max_num_turbs:])\n",
    "# print('\\nLabels to predict:\\n', labels)\n",
    "local_ws =list(df.columns[-2*max_num_turbs:-max_num_turbs])\n",
    "# Helpful variables\n",
    "n_features = len(features)\n",
    "\n",
    "# Convert dataframe to numpy arrays of data\n",
    "X = df[features].values\n",
    "y = df[labels].values\n",
    "\n",
    "# Split data into train/validation/test sets with default 60/20/20 split\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "\n",
    "# Concatenate train and val subsets into one set for post hyperparameter tuning training\n",
    "X_train_val = np.concatenate((X_train, X_val))\n",
    "y_train_val = np.concatenate((y_train, y_val))\n",
    "\n",
    "# # Standardize data\n",
    "# std_scaler = StandardScaler()\n",
    "\n",
    "# X_train_std = std_scaler.fit(X_train).transform(X_train)\n",
    "# X_val_std = std_scaler.fit(X_train).transform(X_val)\n",
    "# X_test_std = std_scaler.fit(X_train).transform(X_test)\n",
    "\n",
    "# X_train_val_std = std_scaler.fit(X_train_val).transform(X_train_val)\n",
    "# X_train_val_test_std = std_scaler.fit(X_train_val).transform(X_test)\n",
    "\n",
    "# Permutate / shuffle turbine specific features within each record of the dataset to increase decision tree generalization\n",
    "t_idx_start = features.index('t_000')\n",
    "t_idx_end = features.index('t_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "t_X_idx_start = features.index('t_X_000')\n",
    "t_X_idx_end = features.index('t_X_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "t_Y_idx_start = features.index('t_Y_000')\n",
    "t_Y_idx_end = features.index('t_Y_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "t_yaw_idx_start = features.index('t_yaw_000')\n",
    "t_yaw_idx_end = features.index('t_yaw_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "t_power_idx_start = labels.index('t_power_000')\n",
    "t_power_idx_end = labels.index('t_power_{x}'.format(x=str(max_num_turbs-1).zfill(3)))\n",
    "\n",
    "X_permu = copy.deepcopy(X)\n",
    "rng = np.random.default_rng(seed=888)\n",
    "X_permu[:,t_idx_start:t_idx_end+1] = rng.permuted(X_permu[:,t_idx_start:t_idx_end+1], axis=1)\n",
    "rng = np.random.default_rng(seed=888)\n",
    "X_permu[:,t_X_idx_start:t_X_idx_end+1] = rng.permuted(X_permu[:,t_X_idx_start:t_X_idx_end+1], axis=1)\n",
    "rng = np.random.default_rng(seed=888)\n",
    "X_permu[:,t_Y_idx_start:t_Y_idx_end+1] = rng.permuted(X_permu[:,t_Y_idx_start:t_Y_idx_end+1], axis=1)\n",
    "rng = np.random.default_rng(seed=888)\n",
    "X_permu[:,t_yaw_idx_start:t_yaw_idx_end+1] = rng.permuted(X_permu[:,t_yaw_idx_start:t_yaw_idx_end+1], axis=1)\n",
    "\n",
    "\n",
    "y_permu = copy.deepcopy(y)\n",
    "rng = np.random.default_rng(seed=888)\n",
    "y_permu[:,t_power_idx_start:t_power_idx_end+1] = rng.permuted(y_permu[:,t_power_idx_start:t_power_idx_end+1], axis=1)\n",
    "\n",
    "#Split data into train/validation/test sets with default 60/20/20 split\n",
    "X_permu_train, X_permu_val, X_permu_test, y_permu_train, y_permu_val, y_permu_test = split_data(X_permu, y_permu)\n",
    "\n",
    "X_permu_train_val = np.concatenate((X_permu_train, X_permu_val))\n",
    "y_permu_train_val = np.concatenate((y_permu_train, y_permu_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree(X_fit, y_fit, X_eval, y_eval, criterion='squared_error', max_depth=None, mode='train', print_output=True, save_model=False, model_name=None):\n",
    "    \"\"\"\n",
    "    Automates the process of creating a decision tree regressor, calculating the mean squared error on training and validation data,\n",
    "    identifying the features used in the model and their importance, and visualizing the decision tree with graphviz \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    criterion : str, {'squared_error', 'friedman_mse', 'absolute_error', 'poisson'}, default='squared_error'\n",
    "        the function to measure the quality of a split in the decision tree regressor.\n",
    "    max_depth : int, default=None\n",
    "        The maximum depth of a the decision tree. If None, then nodes are expanded to minimize the criterion or until all leaves contain less than min_samples_split samples.\n",
    "    mode : str, {'train', 'test'}, default='train'\n",
    "        Flag to indicate whether the model is being trained (evaluate performance on validation data) or tested (evaluated on test data)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mse_fit : float\n",
    "        The mean squared error of the model when predicting the training data. \n",
    "    mse_eval : float\n",
    "        The mean squared error fo the model when predicting the validation data \n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate tree classifier\n",
    "    dt_regr = DecisionTreeRegressor(criterion=criterion, random_state=888, max_depth=max_depth)\n",
    "\n",
    "    # Fit the model to the training data set\n",
    "    dt_regr = dt_regr.fit(X_fit, y_fit)\n",
    "\n",
    "    # Predict the training and validation data \n",
    "    y_fit_pred = dt_regr.predict(X_fit)\n",
    "    y_eval_pred = dt_regr.predict(X_eval)\n",
    "\n",
    "    # Calculate training and validation MSE\n",
    "    mse_fit = calculate_MSEs(y_fit, y_fit_pred)\n",
    "    mse_eval = calculate_MSEs(y_eval, y_eval_pred)\n",
    "\n",
    "    # Get model R2 scores\n",
    "    r2_fit = dt_regr.score(X_fit,y_fit)\n",
    "    r2_eval = dt_regr.score(X_eval,y_eval) \n",
    "\n",
    "    # Get model parameters\n",
    "    dt_depth = dt_regr.get_depth()\n",
    "    dt_n_leaves = dt_regr.get_n_leaves()\n",
    "    dt_parameters = dt_regr.get_params() \n",
    "\n",
    "    # save model if flagged true:\n",
    "    if save_model:\n",
    "        if not os.path.isdir('./saved_decision_tree_models/'):\n",
    "            os.system('mkdir ./saved_decision_tree_models/')\n",
    "        dump_path = './saved_decision_tree_models/'+ model_name + '.joblib'\n",
    "        joblib.dump(dt_regr, dump_path)\n",
    "\n",
    "    if (mode == 'train' and print_output == True):\n",
    "        print('Tree parameters:\\n', dt_parameters)\n",
    "        print('Tree depth:', dt_depth)\n",
    "        print('Number of leaves:', dt_n_leaves)\n",
    "        print('Training Turbine MSE:\\n', mse_fit[0])\n",
    "        print('Training Avg Turbine MSE:\\n', mse_fit[1])\n",
    "        print('Training Plant MSE:\\n', mse_fit[2])\n",
    "        print('Validation Turbine MSE:\\n', mse_eval[0])\n",
    "        print('Validation Avg Turbine MSE:\\n', mse_eval[1])\n",
    "        print('Validation Plant MSE:\\n', mse_eval[2])\n",
    "        print('R2 score training:', r2_fit)\n",
    "        print('R2 score training', r2_eval)  \n",
    "        # Identify and display the features selected by the decision tree model and their importance\n",
    "        df_dt_feature_importance = pd.DataFrame()\n",
    "        df_dt_feature_importance['Feature'] = features\n",
    "        df_dt_feature_importance['Feature Importance'] = dt_regr.feature_importances_\n",
    "        df_dt_feature_importance = df_dt_feature_importance.sort_values(by=['Feature Importance'], ascending=False)\n",
    "        df_dt_feature_importance = df_dt_feature_importance[df_dt_feature_importance['Feature Importance'] > 0]\n",
    "        print('Number of features used in decision tree:', len(df_dt_feature_importance[df_dt_feature_importance['Feature Importance'] > 0]))\n",
    "        print('Features used in the model and their importance:')\n",
    "        display(df_dt_feature_importance)\n",
    "\n",
    "    if (mode == 'test' and print_output == True):\n",
    "        print('Tree parameters:\\n', dt_parameters)\n",
    "        print('Tree depth:', dt_depth)\n",
    "        print('Number of leaves:', dt_n_leaves)\n",
    "        print('Training Turbine MSE:\\n', mse_train_val[0])\n",
    "        print('Training Avg Turbine MSE:\\n', mse_train_val[1])\n",
    "        print('Training Plant MSE:\\n', mse_train_val[2])\n",
    "        print('Test Turbine MSE:\\n', mse_test[0])\n",
    "        print('Test Avg Turbine MSE:\\n', mse_test[1])\n",
    "        print('Test Plant MSE:\\n', mse_test[2])\n",
    "        print('R2 score training:', r2_train_val)\n",
    "        print('R2 score test', r2_test)  \n",
    "        # Identify and display the features selected by the decision tree model and their importance\n",
    "        df_dt_feature_importance = pd.DataFrame()\n",
    "        df_dt_feature_importance['Feature'] = features\n",
    "        df_dt_feature_importance['Feature Importance'] = dt_regr.feature_importances_\n",
    "        df_dt_feature_importance = df_dt_feature_importance.sort_values(by=['Feature Importance'], ascending=False)\n",
    "        df_dt_feature_importance = df_dt_feature_importance[df_dt_feature_importance['Feature Importance'] > 0]\n",
    "        print('Number of features used in decision tree:', len(df_dt_feature_importance[df_dt_feature_importance['Feature Importance'] > 0]))\n",
    "        print('Features used in the model and their importance:')\n",
    "        display(df_dt_feature_importance)\n",
    "        \n",
    "    return mse_fit, mse_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSEs(train, pred):\n",
    "    train_arr = np.array(train)\n",
    "    pred_arr = np.array(pred)\n",
    "    # Turbine level power production, individual turbine MSE across all records in dataset\n",
    "    turb_mse = np.sum(((pred_arr-train_arr)**2), axis=0)/len(train)\n",
    "    # Average turbine MSE\n",
    "    avg_turb_mse = np.sum(turb_mse)/len(turb_mse)\n",
    "    # Plant level power production. Sums all turbine powers to plant level power production and calculate MSE across all records in the dataset\n",
    "    plant_mse = np.sum((np.sum(pred_arr, axis=1)-np.sum(train_arr, axis=1))**2)/len(train)\n",
    "\n",
    "\n",
    "    return [turb_mse, avg_turb_mse, plant_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and load the saved decision tree model from the .joblib file\n",
    "def get_saved_decision_tree(model_name=None):\n",
    "    file_path = './saved_decision_tree_models/'+ model_name + '.joblib'\n",
    "    dt_regr = joblib.load(file_path)\n",
    "\n",
    "    return dt_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Decision Tree (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train, mse_val = build_decision_tree(X_train, y_train, X_val, y_val, save_model=True, model_name='base_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Permuted Decision Tree (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_permu_train, mse_permu_val = build_decision_tree(X_permu_train, y_permu_train, X_permu_val, y_permu_val, save_model=True, model_name='base_permu_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decision tree regressor from .joblib file\n",
    "dt_regr = get_saved_decision_tree(model_name='base_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune max_depth of baseline tree\n",
    "# parameter sweep of decision tree parameters\n",
    "dt_hyp_opt_column_names = ['Tree Depth', 'Training Avg Turbine MSE', 'Validation Avg Turbine MSE', 'Training Plant MSE', 'Validation Plant MSE' ]\n",
    "\n",
    "max_depths = np.arange(25,225,25)\n",
    "# criterion = ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']\n",
    "\n",
    "# criterion_depth_combos = list(itertools.product(criterion, max_depths))\n",
    "\n",
    "results_list_mses = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    print(\"==========================\")\n",
    "    print(\"Hyperparameters\")\n",
    "    print(\"max_depth =\", depth)\n",
    "    print(\"==========================\")\n",
    "    mse_train, mse_val = build_decision_tree(X_train, y_train, X_val, y_val, max_depth=depth, print_output=False)\n",
    "    results_list_mses.append([depth, mse_train[1], mse_val[1], mse_train[2], mse_val[2]])\n",
    "\n",
    "# df_dt_hyp_opt = pd.DataFrame(data=results_list, columns=dt_hyp_opt_column_names, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune max_depth of baseline permuted tree\n",
    "# parameter sweep of decision tree parameters\n",
    "dt_hyp_opt_column_names = ['Tree Depth', 'Training Avg Turbine MSE', 'Validation Avg Turbine MSE', 'Training Plant MSE', 'Validation Plant MSE' ]\n",
    "\n",
    "max_depths = np.arange(25,225,25)\n",
    "# criterion = ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']\n",
    "\n",
    "# criterion_depth_combos = list(itertools.product(criterion, max_depths))\n",
    "\n",
    "results_list_permu_mses = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    print(\"==========================\")\n",
    "    print(\"Hyperparameters\")\n",
    "    print(\"max_depth =\", depth)\n",
    "    print(\"==========================\")\n",
    "    mse_permu_train, mse_permu_val = build_decision_tree(X_permu_train, y_permu_train, X_permu_val, y_permu_val, max_depth=depth, print_output=False)\n",
    "    results_list_permu_mses.append([depth, mse_permu_train[1], mse_permu_val[1], mse_permu_train[2], mse_permu_val[2]])\n",
    "\n",
    "# df_dt_hyp_opt = pd.DataFrame(data=results_list, columns=dt_hyp_opt_column_names, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt_hyp_opt = pd.DataFrame(data=results_list_mses, columns=dt_hyp_opt_column_names)\n",
    "\n",
    "# Overall the decision tree model does not appear to generalize to new unseen data (validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Decision Tree (trained on X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tree classifier\n",
    "dt_regr = DecisionTreeRegressor(criterion='squared_error', random_state=888, max_depth=100)\n",
    "\n",
    "# Fit the model to the training data set\n",
    "dt_regr = dt_regr.fit(X_train, y_train)\n",
    "\n",
    "# save model\n",
    "dump_path = './saved_decision_tree_models/'\n",
    "if os.path.isdir(dump_path):\n",
    "    os.system('mkdir {}'.format(dump_path))\n",
    "dump_path += ('tuned_dt_train' + '.joblib')\n",
    "joblib.dump(dt_regr, dump_path)\n",
    "\n",
    "# Predict the training, validation, and test data \n",
    "y_train_pred = dt_regr.predict(X_train)\n",
    "y_val_pred = dt_regr.predict(X_val)\n",
    "y_test_pred = dt_regr.predict(X_test)\n",
    "\n",
    "# Calculate training and validation MSE\n",
    "mse_train = [mean_squared_error(y_train, y_train_pred, multioutput='raw_values'),\n",
    "                mean_squared_error(y_train, y_train_pred, multioutput='uniform_average')]\n",
    "mse_val = [mean_squared_error(y_val, y_val_pred, multioutput='raw_values'),\n",
    "            mean_squared_error(y_val, y_val_pred, multioutput='uniform_average')]\n",
    "mse_test = [mean_squared_error(y_test, y_test_pred, multioutput='raw_values'),\n",
    "            mean_squared_error(y_test, y_test_pred, multioutput='uniform_average')]\n",
    "\n",
    "print('Tree parameters:\\n', dt_regr.get_params())\n",
    "print('Tree depth:', dt_regr.get_depth())\n",
    "print('Number of leaves:', dt_regr.get_n_leaves())\n",
    "print('Training MSE:\\n', mse_train)\n",
    "print('Validation MSE:\\n', mse_val)\n",
    "print('Test MSE:\\n', mse_test)\n",
    "print('R2 score training:', dt_regr.score(X_train,y_train))\n",
    "print('R2 score validation', dt_regr.score(X_val,y_val))\n",
    "print('R2 score test', dt_regr.score(X_test,y_test))   \n",
    " # Identify and display the features selected by the decision tree model and their importance\n",
    "df_dt_feature_importance = pd.DataFrame()\n",
    "df_dt_feature_importance['Feature'] = features\n",
    "df_dt_feature_importance['Feature Importance'] = dt_regr.feature_importances_\n",
    "df_dt_feature_importance = df_dt_feature_importance.sort_values(by=['Feature Importance'], ascending=False)\n",
    "df_dt_feature_importance = df_dt_feature_importance[df_dt_feature_importance['Feature Importance'] > 0]\n",
    "print('Number of features used in decision tree:', len(df_dt_feature_importance[df_dt_feature_importance['Feature Importance'] > 0]))\n",
    "print('Features used in the model and their importance:')\n",
    "display(df_dt_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Decision Tree (trained on X_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate tree classifier\n",
    "dt_regr = DecisionTreeRegressor(criterion='squared_error', random_state=888, max_depth=100)\n",
    "\n",
    "# Fit the model to the training data set\n",
    "dt_regr = dt_regr.fit(X_train_val, y_train_val)\n",
    "\n",
    "# save model\n",
    "dump_path = './saved_decision_tree_models/'+ 'tuned_dt_train_val' + '.joblib'\n",
    "joblib.dump(dt_regr, dump_path)\n",
    "\n",
    "# Predict the training, validation, and test data \n",
    "y_train_val_pred = dt_regr.predict(X_train_val)\n",
    "y_test_pred = dt_regr.predict(X_test)\n",
    "\n",
    "# Calculate training and validation MSE\n",
    "mse_train_val = [mean_squared_error(y_train_val, y_train_val_pred, multioutput='raw_values'),\n",
    "                mean_squared_error(y_train_val, y_train_val_pred, multioutput='uniform_average')]\n",
    "mse_test = [mean_squared_error(y_test, y_test_pred, multioutput='raw_values'),\n",
    "            mean_squared_error(y_test, y_test_pred, multioutput='uniform_average')]\n",
    "\n",
    "print('Tree parameters:\\n', dt_regr.get_params())\n",
    "print('Tree depth:', dt_regr.get_depth())\n",
    "print('Number of leaves:', dt_regr.get_n_leaves())\n",
    "print('Training+Validation MSE:\\n', mse_train_val)\n",
    "print('Test MSE:\\n', mse_test)\n",
    "print('R2 score training+validation:', dt_regr.score(X_train_val,y_train_val))\n",
    "print('R2 score test', dt_regr.score(X_test,y_test) )   \n",
    " # Identify and display the features selected by the decision tree model and their importance\n",
    "df_dt_feature_importance = pd.DataFrame()\n",
    "df_dt_feature_importance['Feature'] = features\n",
    "df_dt_feature_importance['Feature Importance'] = dt_regr.feature_importances_\n",
    "df_dt_feature_importance = df_dt_feature_importance.sort_values(by=['Feature Importance'], ascending=False)\n",
    "df_dt_feature_importance = df_dt_feature_importance[df_dt_feature_importance['Feature Importance'] > 0]\n",
    "print('Number of features used in decision tree:', len(df_dt_feature_importance[df_dt_feature_importance['Feature Importance'] > 0]))\n",
    "print('Features used in the model and their importance:')\n",
    "display(df_dt_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have demonstrated a potential use case of the floris_data.h5 data in an off-the-shelf sklearn decision tree model. As seen above the decision tree models perform poorly and are unsuccessful at generalizing well to new data. The performance seen from the decision tree models is to be expected. There are inherent limitations in how the data is structured and the algorithms used. By one-hot encoding the turbine information we impose an arbitrary ordering in the turbine related variables that is not representative of an actual ordering of turbines in the generated layouts. For example turbine 000 may be at the forefront of inflow and experience no wake effects in one layout 000 and while in layout 499 the turbine is nested deep into a cluster with different inflow conditions and experiences heavy wake effects. Thus, the model will struggle to generalize to new data with different turbine placement. In an effort to overcome this issue we randomly permutated the order of turbine related variables, however model performance did not increase. Additionally, the logic within a decision tree algorithm splits nodes based on minimizing mean squared error of the predictions. Inherently weak learner models such as decision trees will struggle to capture the complex interactions of wake effects between multiple turbines. A nueral network would be better suited to capture those relationships. We put in the effort to spin up this off-the-shelf model to showcase the utility and increase in performance when implementing a graph neural network (GNN) to capture wake effects and predict wind plant power output. The WPGNN model developed by NREL delivers superior accuracy and speed to these off-the-shelf architectures.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
